---
title: 机器学习中常见的损失函数
date: 2025-06-10 14:48:08
tags:
- Loss Functions
categories:
- Artificial Intelligence
- Machine Learning
math: true
---

## 回归任务损失函数(Regression Losses)

### 均方误差(MSE, Mean Squared Error)

均方误差是回归任务中最常用的损失函数之一。它计算预测值与实际值之间差异的平方和的平均值。公式如下：

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

其中，$y_i$ 是实际值，$\hat{y}_i$ 是预测值，$n$ 是样本数量。

MSE 的优点是对大误差有较强的惩罚作用，因为误差被平方了。这使得模型在训练时更倾向于减少大误差。
然而，MSE 对异常值非常敏感，因为异常值的平方会显著增加总损失。

### 平均绝对误差(MAE, Mean Absolute Error)

平均绝对误差是另一种常用的回归损失函数。它计算预测值与实际值之间差异的绝对值的平均值。公式如下：

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

MAE 的优点是对异常值不如 MSE 敏感，因此在数据中存在异常值时，MAE 可能更合适。然而，MAE 在梯度下降时可能导致不稳定的梯度，因为绝对值函数在零点处不可导。

### Huber 损失函数
Huber 损失函数结合了 MSE 和 MAE 的优点。它在误差小于某个阈值时使用 MSE，在误差大于该阈值时使用 MAE。公式如下：

$$
\text{Huber} =
\begin{cases}
\frac{1}{2}(y_i - \hat{y}_i)^2 & \text{if } |y_i - \hat{y}_i| \leq \delta \\
\delta \cdot (|y_i - \hat{y}_i| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

其中，$\delta$ 是一个超参数，用于控制 MSE 和 MAE 的切换点。
Huber 损失函数在处理异常值时表现良好，因为它在误差较大时不会像 MSE 那样过于敏感。

## 分类任务损失函数(Classification Losses)

### 交叉熵损失(Cross-Entropy Loss)

交叉熵损失是分类任务中最常用的损失函数。它衡量了预测分布与实际分布之间的差异。对于二分类问题，交叉熵损失的公式如下：

$$
\text{Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$

对于多分类问题，交叉熵损失的公式为：

$$
\text{Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(\hat{y}_{ij})
$$

其中，$y_i$ 是实际标签，$\hat{y}_i$ 是预测概率，$k$ 是类别数量。
交叉熵损失的优点是它对概率分布的差异非常敏感，因此在训练分类模型时效果很好。它也具有良好的数学性质，使得梯度下降算法能够有效地优化模型参数。

### 二元交叉熵损失(Binary Cross-Entropy Loss)
二元交叉熵损失是交叉熵损失的特例，专门用于二分类问题。它的公式与上述二分类交叉熵损失相同，但通常使用 Sigmoid 函数将输出转换为概率值。

### 类别平衡交叉熵损失(Class-Balanced Cross-Entropy Loss)
类别平衡交叉熵损失是一种改进的交叉熵损失，旨在处理类别不平衡问题。它通过为每个类别分配不同的权重来平衡损失。公式如下：

$$
\text{Class-Balanced Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} w_{y_i} \log(\hat{y}_{i})
$$

其中，$w_{y_i}$ 是类别 $y_i$ 的权重，通常根据类别的频率进行计算。这样可以减少对少数类的忽视，提高模型在不平衡数据集上的性能。

### Focal Loss
Focal Loss 是一种用于处理类别不平衡问题的损失函数。它在交叉熵损失的基础上引入了一个调节因子，使得模型更关注难以分类的样本。公式如下：

$$
\text{Focal Loss} = -\frac{1}{n} \sum_{i=1}^{n} (1 - \hat{y}_i)^\gamma y_i \log(\hat{y}_i)
$$

其中，$\gamma$ 是一个超参数，用于控制调节因子的强度。当 $\gamma = 0$ 时，Focal Loss 等同于交叉熵损失。随着 $\gamma$ 的增大，模型对难以分类的样本的关注程度增加。

### Kullback-Leibler 散度(KL Divergence)
Kullback-Leibler散度是一种衡量两个概率分布之间差异的损失函数。它通常用于变分自编码器等模型中。公式如下：

$$
\text{KL Divergence} = \sum_{i=1}^{n} y_i \log\left(\frac{y_i}{\hat{y}_i}\right)
$$

KL散度的优点是它可以处理连续分布，并且在概率分布之间的差异较大时具有较强的惩罚作用。然而，它对零概率事件非常敏感，因此在实际应用中需要小心处理。

## 其他常见损失函数

### Wasserstein距离

Wasserstein距离是一种衡量两个概率分布之间差异的距离度量。它基于最优传输理论，能够更好地捕捉分布之间的差异，尤其是在高维空间中。

Wasserstein 距离的定义如下:

$$
\text{Wasserstein\_Distance}(P, Q) = \inf_{\gamma \in \Gamma(P, Q)} \int_{X \times X} c(x, y) \operatorname{d}\gamma(x, y)
$$

其中，$P$ 和 $Q$ 是两个概率分布，$\Gamma(P, Q)$ 是所有可能的联合分布，$c(x, y)$ 是成本函数，表示从 $x$ 到 $y$ 的传输成本。
Wasserstein 距离的优点是它具有更好的数学性质，能够处理分布之间的细微差异，并且在高维空间中表现良好。

而且相比于KL散度，Wasserstein是真正的距离度量，满足三角不等式和对称性。

### Wasserstein 损失函数(Wasserstein Loss)

Wasserstein损失函数是一种用于生成对抗网络(GAN)的损失函数。它基于 Wasserstein距离，能够更好地处理生成模型中的模式崩溃问题。公式如下：

$$
\text{Wasserstein Loss} = \mathbb{E}[D(x)] - \mathbb{E}[D(G(z))]
$$

其中，$D(x)$ 是判别器对真实样本的评分，$G(z)$ 是生成器生成的样本。Wasserstein 损失函数的优点是它具有更好的梯度性质，使得 GAN 的训练更加稳定。

